# Adaptive RAG: Intelligent Retrieval System

## ğŸ¯ Project Goal

Build an **intelligent retrieval system that adapts to queries** using dynamic chunking, hybrid retrieval, and reranking to achieve measurable improvements in both precision and latency.

## ğŸ† Target Outcome

A **modular, production-ready RAG (Retrieval-Augmented Generation) pipeline** that is:
- Benchmarked on accuracy, speed, and efficiency
- Reproducible and well-documented
- Adaptable to different query types and use cases

## ğŸ’¡ Use Cases

This system is designed for scenarios where you need to:

1. **Question Answering Systems**: Find the most relevant documents to answer user questions
   - Customer support chatbots
   - Internal knowledge base search
   - Research paper retrieval

2. **Semantic Search**: Go beyond keyword matching to understand query intent
   - E-commerce product search
   - Legal document retrieval
   - Medical literature search

3. **RAG Applications**: Provide relevant context to LLMs for better responses
   - AI assistants with company knowledge
   - Code search and documentation
   - Academic research helpers

## ğŸš€ Quick Start

### Installation

```bash
# Install dependencies with Poetry
poetry install

# Or with pip (if not using Poetry)
pip install -e .
```

### Basic Usage

```bash
# 1. Build the search index (downloads MS MARCO and creates embeddings)
python scripts/build_index.py

# 2. Evaluate the retrieval system
python scripts/evaluate.py

# 3. Try interactive search
python scripts/retrieve.py
```

## ğŸ“Š Project Milestones

### âœ… Milestone 1 â€” Retrieval Foundations (Current)

**What**: Build a baseline dense retrieval system using semantic embeddings.

**Why**: Establish a solid foundation and baseline metrics before adding complexity.

**Deliverable**: 
- Working retrieval pipeline with FAISS vector database
- Evaluation metrics (Recall@k, nDCG)
- Baseline performance benchmarks

**[See detailed documentation â†’](docs/MILESTONE_1.md)**

### ğŸ”œ Milestone 2 â€” Dynamic Chunking Engine

Implement adaptive text chunking based on query characteristics and semantic boundaries.

### ğŸ”œ Milestone 3 â€” Hybrid Retrieval Layer

Combine dense (semantic) and sparse (keyword) retrieval methods for better coverage.

### ğŸ”œ Milestone 4 â€” Re-Ranker & Adaptive Policy

Add cross-encoder reranking and intelligent decision-making for when to use different strategies.

### ğŸ”œ Milestone 5 â€” Benchmark & Optimization

Comprehensive evaluation with latency profiling and visualization dashboard.

## ğŸ“ˆ Success Criteria

- â‰¥ 15% improvement in nDCG@10 over dense baseline
- Median latency â‰¤ 500ms with rerank enabled
- Modular, maintainable codebase

## ğŸ“ Project Structure

```
adaptative-rag/
â”œâ”€â”€ configs/                    # Hydra configuration files
â”‚   â”œâ”€â”€ config.yaml            # Main config
â”‚   â”œâ”€â”€ data/                  # Dataset configs
â”‚   â”œâ”€â”€ model/                 # Model configs
â”‚   â””â”€â”€ retrieval/             # Retrieval configs
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ data/                  # Data loading and preprocessing
â”‚   â”œâ”€â”€ retrieval/             # Retrieval components
â”‚   â”œâ”€â”€ evaluation/            # Metrics and evaluation
â”‚   â””â”€â”€ utils/                 # Shared utilities
â”œâ”€â”€ scripts/                   # Entry point scripts
â”‚   â”œâ”€â”€ build_index.py         # Build search index
â”‚   â”œâ”€â”€ evaluate.py            # Run evaluation
â”‚   â””â”€â”€ retrieve.py            # Interactive search
â”œâ”€â”€ tests/                     # Unit tests
â”œâ”€â”€ docs/                      # Documentation
â””â”€â”€ data/                      # Data storage (created on first run)
```

## ğŸ› ï¸ Technology Stack

### Core Libraries
- **sentence-transformers**: Neural text embeddings
- **FAISS**: Fast similarity search
- **Hydra**: Configuration management
- **Poetry**: Dependency management

### Dataset
- **MS MARCO**: Microsoft MAchine Reading COmprehension dataset
  - ~8.8M passages
  - ~6.9k evaluation queries
  - Industry-standard benchmark

## ğŸ“– Documentation

- **[Milestone 1 Guide](docs/MILESTONE_1.md)**: Detailed walkthrough of the current system
- **[Architecture](docs/ARCHITECTURE.md)**: System design and components
- **[Configuration](docs/CONFIGURATION.md)**: How to customize settings

## ğŸ§ª Running Tests

```bash
# Run all tests
poetry run pytest

# Run with coverage
poetry run pytest --cov=src

# Run specific test file
poetry run pytest tests/test_metrics.py
```

## ğŸ“ Development

### Code Quality

```bash
# Format code
poetry run black src/ tests/

# Lint code
poetry run ruff check src/ tests/
```

### Configuration

All settings are managed through Hydra configs in `configs/`. You can:
- Override any parameter from command line
- Create new config variants
- Compose multiple configs

Example:
```bash
# Use different model
python scripts/build_index.py model.name=all-mpnet-base-v2

# Change top_k results
python scripts/evaluate.py retrieval.top_k=20
```

## ğŸ¤ Contributing

This is a learning/research project. Feel free to:
- Experiment with different models
- Add new evaluation metrics
- Optimize performance
- Improve documentation

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

- MS MARCO dataset by Microsoft
- sentence-transformers by UKP Lab
- FAISS by Facebook Research
